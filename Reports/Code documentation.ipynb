{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Exploratory Data Analysis (EDA) Report\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This report provides a detailed exploratory data analysis (EDA) of the given dataset. The primary objectives of this EDA are to:\n",
    "\n",
    "- Understand the underlying structure of the data.\n",
    "- Identify patterns, anomalies, and relationships within the dataset.\n",
    "- Prepare the data for further modeling by cleaning and transforming it.\n",
    "\n",
    "We will cover various aspects of the EDA process, including data loading, data cleaning, univariate, bivariate, and multivariate analysis, and Principal Component \n",
    "Analysis (PCA).\n",
    "\n",
    "## Core Feature\n",
    "In the duration  of our project, our primary goal has been making the processes \n",
    "MODULAR, and FUNCTIONALISED, focusing on catering to the entire dataset at any given step. We aim to standardise the process through this technique\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "In this section, we will load the dataset and perform an initial examination to understand its structure and basic characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('File Path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('File Path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Data cleaning is a crucial step to ensure the quality of the dataset. In this section, we will handle missing values and detect and treat outliers.\n",
    "\n",
    "### Null Handling\n",
    "\n",
    "We will address any missing values in the dataset using appropriate strategies such as imputation , using various techniques, focusing on maximizing the quality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def smart_impute(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Function to determine if a column is numeric\n",
    "    def is_numeric(col):\n",
    "        return pd.api.types.is_numeric_dtype(df[col])\n",
    "    \n",
    "    # Function to determine if a column is datetime\n",
    "    def is_datetime(col):\n",
    "        return pd.api.types.is_datetime64_any_dtype(df[col])\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0:  # Check if column has missing values\n",
    "            if is_numeric(column):\n",
    "                # For numeric columns, use KNN imputation\n",
    "                imputer = KNNImputer(n_neighbors=5)\n",
    "                df[column] = imputer.fit_transform(df[[column]])\n",
    "            elif is_datetime(column):\n",
    "                # For datetime columns, forward fill and then backward fill\n",
    "                df[column] = df[column].fillna(method='ffill').fillna(method='bfill')\n",
    "            else:\n",
    "                # For categorical/text columns, use mode\n",
    "                df[column] = df[column].fillna(df[column].mode()[0])\n",
    "    \n",
    "    # Save the processed DataFrame back to the original file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated original file: {file_path}\")\n",
    "    \n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Handling\n",
    "\n",
    "We will identify outliers using statistical methods such as the Interquartile Range (IQR) and Z-score and decide on appropriate treatment methods to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def handle_outliers(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    for column in df.select_dtypes(include=[np.number]).columns:\n",
    "        # Check for normality\n",
    "        _, p_value = stats.normaltest(df[column].dropna())\n",
    "        \n",
    "        if p_value < 0.05:  # Not normally distributed\n",
    "            # Use IQR method\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df[column] = df[column].clip(lower_bound, upper_bound)\n",
    "            print(f\"Used IQR method for {column}\")\n",
    "        else:  # Normally distributed\n",
    "            # Use Z-score method\n",
    "            z_scores = np.abs(stats.zscore(df[column]))\n",
    "            df[column] = df[column].mask(z_scores > 3, df[column].median())\n",
    "            print(f\"Used Z-score method for {column}\")\n",
    "        \n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Handled outliers in {file_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "In this section, we'll perform an exploratory data analysis (EDA) on our cleaned dataset. EDA is a critical step in understanding our data's characteristics, patterns, and relationships. We'll divide our analysis into two phases:\n",
    "\n",
    "1. Analysis by columns (pre-PCA)\n",
    "2. Analysis post-PCA (Principal Component Analysis)\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# Use Agg backend for Matplotlib\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries will help us load, manipulate, and visualize our data effectively.\n",
    "\n",
    "## Analysis by columns (pre-PCA)\n",
    "\n",
    "In this phase, we'll analyze our data based on its original columns. This helps us understand the basic structure and characteristics of our dataset.\n",
    "\n",
    "### Univariate Analysis\n",
    "We'll perform univariate analysis, which examines each variable in the dataset individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_univariate(df, column, ax):\n",
    "    sns.histplot(df[column], kde=True, ax=ax)\n",
    "    ax.set_title(column)\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates histograms for each numerical column. Histograms show the distribution of values in each column, helping us identify patterns like normal distributions, skewness, or outliers.\n",
    "\n",
    "### Bivariate Analysis\n",
    "Bivariate analysis examines the relationship between pairs of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_bivariate(df, col1, col2, ax):\n",
    "    sns.scatterplot(data=df, x=col1, y=col2, ax=ax)\n",
    "    ax.set_title(f'{col1} vs {col2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates scatter plots for pairs of numerical variables. Scatter plots help us visualize relationships between variables, such as positive or negative correlations, or more complex patterns.\n",
    "\n",
    "### Multivariate Analysis\n",
    "A correlation heatmap provides an overview of relationships between all pairs of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_correlation_heatmap(df, prefix=\"correlation_heatmap\"):\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'{prefix.replace(\"_\", \" \").title()}')\n",
    "    plt.savefig(f'plots/{prefix}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap displays correlation coefficients between -1 and 1, where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation. This helps us quickly identify which variables are most strongly related to each other.\n",
    "\n",
    "## Phase 2: Analysis Post-PCA\n",
    "In this phase, we'll perform Principal Component Analysis (PCA) and analyze the results. PCA is a technique used to reduce the dimensionality of a dataset while retaining as much of the original variability as possible.\n",
    "\n",
    "### PCA\n",
    "**PCA** works by finding new variables (principal components) that are linear combinations of the original variables. These new variables are uncorrelated and ordered so that the first few retain most of the variation present in all of the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions_with_pca(df, n_components=2):\n",
    "    scaler = StandardScaler()\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    scaled_data = scaler.fit_transform(numeric_df)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(scaled_data)\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(n_components)], index=numeric_df.columns)\n",
    "    \n",
    "    top_features_dict = {f'PC{i+1}': ', '.join(loadings[f'PC{i+1}'].abs().sort_values(ascending=False).head(3).index.tolist()) for i in range(n_components)}\n",
    "    \n",
    "    print(f\"Explained variance ratio by PCA: {explained_variance}\")\n",
    "    return pd.concat([df.reset_index(drop=True), pca_df], axis=1), loadings, top_features_dict, explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function standardizes our data, performs PCA, and returns the transformed data along with information about the principal components.\n",
    "\n",
    "### Analyzing PCA Results\n",
    "**Univariate and Bivariate Analysis of PCA Components**\n",
    "We'll perform the same univariate and bivariate analyses on our PCA components as we did on the original variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform univariate and bivariate analysis on PCA components\n",
    "pca_columns = [col for col in pca_df.columns if col.startswith('PC')]\n",
    "combined_analysis(pca_df, pca_columns, plot_univariate, prefix=\"univariate_pca\")\n",
    "combined_analysis(pca_df, pca_columns, plot_bivariate, prefix=\"bivariate_pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us understand the distribution and relationships of our new principal components.\n",
    "\n",
    "**Multivariate Analysis of PCA Components** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap for PCA components\n",
    "plot_correlation_heatmap(pca_df[pca_columns], prefix=\"pca_correlation_heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap should show little to no correlation between components, as PCA creates uncorrelated variables.\n",
    "\n",
    "### Explained Variance Plot\n",
    "The explained variance plot shows how much of the total variance in the dataset is explained by each principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(explained_variance):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(explained_variance) + 1), explained_variance, align='center', alpha=0.8)\n",
    "    plt.title('Explained Variance Ratio by Principal Components')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.xticks(range(1, len(explained_variance) + 1))\n",
    "    plt.grid(True)\n",
    "    plt.savefig('explained_variance.png')  # Save the figure to a file\n",
    "    plt.close()  # Close the figure to release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps us determine how many principal components are needed to explain a significant portion of the variance in our data.\n",
    "\n",
    "### Biplot of First Two Principal Components\n",
    "A biplot allows us to visualize both the principal components and the original features in the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(pca_df, loadings):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=pca_df, x='PC1', y='PC2')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('Biplot of PC1 and PC2')\n",
    "    \n",
    "    for i in loadings.index:\n",
    "        plt.arrow(0, 0, loadings.loc[i, 'PC1'], loadings.loc[i, 'PC2'], color='r', alpha=0.5)\n",
    "        plt.text(loadings.loc[i, 'PC1']*1.1, loadings.loc[i, 'PC2']*1.1, i, color='g', ha='center', va='center')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.savefig('plots/biplot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how the original features contribute to the first two principal components and how the data points are distributed in this new space.\n",
    "\n",
    "\n",
    "### Interpreting PCA Results\n",
    "Finally, we'll look at the PCA loadings and top contributing features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPCA Loadings (Feature Contributions):\")\n",
    "print(loadings)\n",
    "\n",
    "print(\"\\nTop Contributing Features per Principal Component:\")\n",
    "for pc, features in top_features_dict.items():\n",
    "    print(f\"{pc}: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadings show how much each original feature contributes to each principal component. The top contributing features give us a quick summary of which original features are most important for each principal component.\n",
    "\n",
    "## Conclusion\n",
    "This two-phase analysis provides a comprehensive view of our data. In Phase 1, we examined the original features, their distributions, and relationships, giving us a ground-level understanding of our dataset. In Phase 2, we used PCA to reduce dimensionality and potentially uncover hidden patterns in the data.\n",
    "\n",
    "The pre-PCA analysis helps us understand individual features and their direct relationships, while the post-PCA analysis provides insights into the overall structure of the data and the most important combinations of features that explain the variance in our dataset.\n",
    "By comparing the results from both phases, we can gain a deeper understanding of our data's structure and the underlying patterns that might not be immediately apparent from the original variables alone.\n",
    "\n",
    "**NOTE**\n",
    "- All the code for the processes can be found in their respective folders in the repository.\n",
    "- The detailed analysis reports of the dataset can be found under the Reports folder, where this file lies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
